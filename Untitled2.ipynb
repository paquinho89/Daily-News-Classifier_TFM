{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the model and analyzing the most frequent words in a set of news obtained from a media organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get the set of news, the API news is going to be used. This API is available for Python and collects news depending on the media organization, date, language...\n",
    "#### For more information related to this API see the below link:\n",
    "#### ................................................................................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To used the API the below package should be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install newsapi-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data used to train the model\n",
    "path_file= './data/dataset_news.csv'\n",
    "file=pd.read_csv(path_file, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function is created to normalize the text, removing punctuation symbols and double spaces. This cleaned data is \n",
    "#saved in a new column ('TEXT').\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove punctuation that is not word-internal (e.g., hyphens, apostrophes)\n",
    "    text = re.sub('\\s\\W',' ',text)\n",
    "    text = re.sub('\\W\\s',' ',text)\n",
    "    \n",
    "    # make sure we didn't introduce any double spaces\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "file['TEXT'] = [normalize_text(text) for text in file['TITLE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The text is going to be vectorized.  For more info see the below link:\n",
    "#http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(file['TEXT'])\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(file['CATEGORY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92528939706\n",
      "0.926993821462\n",
      "0.926188954383\n",
      "0.926615060483\n",
      "0.927370863122\n",
      "0.927015766299\n",
      "0.928294114862\n",
      "0.928008333136\n",
      "0.926895670084\n",
      "0.926537086579\n",
      "The mean of the whole predictions is: 0.926920906747\n"
     ]
    }
   ],
   "source": [
    "### the Naive Bayes model e dividimos o data set co StrtifiedKfold polas razóns da explicación de arriba\n",
    "### the dta is splitted using the Strtified K fold.\n",
    "kf = StratifiedKFold(n_splits=10,shuffle=True,random_state=42)\n",
    "i=1\n",
    "score_mean=[]\n",
    "\n",
    "for train_index,test_index in kf.split(X,y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    #Naive Bayes model\n",
    "    nb = MultinomialNB()\n",
    "    classifier_model = nb.fit(X_train, y_train)\n",
    "    score = nb.score(X_test, y_test)\n",
    "    score_mean.append(score)\n",
    "    print(score)\n",
    "    i+=1\n",
    "print('The mean of the whole predictions is:',sum(score_mean)/len(score_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the News Api for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The api_key is the key from my account, which can be used to check the project.\n",
    "newsapi = NewsApiClient(api_key='9fe0d6dd387c40bc8cb5fdec346f0bda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "today= str(date.today())\n",
    "yesterday = str(date.today()-timedelta(1))\n",
    "\n",
    "#For more information related to number\n",
    "\n",
    "all_articles = newsapi.get_everything(sources='bbc-news',\n",
    "                                      domains='bbc.co.uk,techcrunch.com',\n",
    "                                      from_param= yesterday,\n",
    "                                      to= today,\n",
    "                                      language='en',\n",
    "                                      sort_by='relevancy',\n",
    "                                      page=4)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To see the available media sources\n",
    "sources = newsapi.get_sources()\n",
    "sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values of the data set obatined from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The news where the 'content' field is None, are going to be deleted from the data set.\n",
    "#This approach was followed due to the problems with the None values when the news are to be classified.\n",
    "\n",
    "#This code calculates the number of news with none content.\n",
    "#When there is content, this field is a string, so following this assupmtion the number of none values is calculated.\n",
    "n=0\n",
    "for i in range(len(all_articles['articles'])):\n",
    "    if type(all_articles['articles'][i]['content'])!=str:\n",
    "        n=n+1\n",
    "#This code deletes the None values.\n",
    "#Note that the n (number of missing values) is subtracted from the len of the array, due to if 'None' values are deleted,\n",
    "#the len of the array changes. Following this way, the len is stablished correctly from the beginning.\n",
    "\n",
    "#The code has to be runned 2 times. To be honest, I do not know the reason. I just know that running the code 2 times,\n",
    "#no error pops up.\n",
    "for l in range(2):\n",
    "    for i in range(len(all_articles['articles'])-n):\n",
    "        if type(all_articles['articles'][i]['content'])!=str:\n",
    "            del(all_articles['articles'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_articles['articles'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([\"business\", \"science and technology\", \"entertainment\", \"health\"])\n",
    "\n",
    "x_axis=[]\n",
    "\n",
    "corpus_business=''\n",
    "corpus_sciencetechnology=''\n",
    "corpus_entertainment=''\n",
    "corpus_health=''\n",
    "\n",
    "#The classification of the news is performed. Thus, each news is placed in its proper corpus.\n",
    "#In case there are News with a None in their content, the code will not run.\n",
    "for i in range(len(all_articles['articles'])):\n",
    "    prediction = nb.predict(vectorizer.transform([all_articles['articles'][i]['content']]))\n",
    "    x_axis.append(list(le.inverse_transform(prediction)))\n",
    "    #Each news is placed in its corpus.\n",
    "    if list(le.inverse_transform(prediction)) == ['entertainment']:\n",
    "        corpus_business= corpus_business + '' + all_articles['articles'][i]['content']\n",
    "    if list(le.inverse_transform(prediction)) == ['science and technology']:\n",
    "        corpus_sciencetechnology= corpus_sciencetechnology + '' + all_articles['articles'][i]['content']\n",
    "    if list(le.inverse_transform(prediction)) == ['entertainment']:\n",
    "        corpus_entertainment= corpus_entertainment + '' + all_articles['articles'][i]['content']\n",
    "    if list(le.inverse_transform(prediction)) == ['health']:\n",
    "        corpus_health= corpus_health + '' + all_articles['articles'][i]['content']\n",
    "\n",
    "corpus=[corpus_business,corpus_sciencetechnology,corpus_entertainment,corpus_health]\n",
    "\n",
    "#Metemos os nosos datos nunha única lista, sen listas polo medio.\n",
    "\n",
    "x_axis_list=[]\n",
    "for i in range(len(x_axis)):\n",
    "    x_axis_list.append(x_axis[i][0])\n",
    "#Contamos as ocurrencias de cada tipo que aparece na base.\n",
    "x=Counter(x_axis_list)\n",
    "\n",
    "#NÚMERO DE NOTICIAS. de dnde sn princialmente las noticias. Representamso os datos.\n",
    "labels, values = zip(*Counter(x_axis_list).items())\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "plt.bar(indexes, values)\n",
    "plt.xticks(indexes , labels, rotation= 'vertical')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf (*term frequency*)\n",
    "\n",
    "[**tf**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Term_frequency_2) es el peso que indica la frecuencia de un término, es decir, el número de veces que una determinada palabra aparece en un documento. \n",
    "\n",
    "La aproximación más sencilla consiste consiste en asignar como peso para el término $t$ en el documento $d$ del corpus $D$ (denotado como $\\mbox{tf}_{t,d}$) el número de ocurrencias de $t$ en $d$. Es recomendable normalizar esta frecuencia, diviendo el número de ocurrencias entre el número total de palabras de un documento, para no penalizar los documentos breves: $\\mathrm{tf}(t,d) = \\frac{\\mathrm{f}(t, d)}{\\max\\{\\mathrm{f}(w, d):w \\in d\\}}$\n",
    "\n",
    "Vamos a calcularlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf={}\n",
    "i=-1\n",
    "for text in corpus:\n",
    "    sentence=text.split()\n",
    "    for word in sentence:\n",
    "        tf[word]=[0]*len(corpus)\n",
    "\n",
    "for text in corpus:\n",
    "    sentence=text.split()\n",
    "    i= i+1\n",
    "    for word in sentence:\n",
    "        tf[word][i]=sentence.count(word)/len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Image copyright RMT Image caption Further 24-hour strikes will take place on SWR on consecutive Saturdays from 3-24 November Staff on South Western Railway (SWR) have begun a five-day strike over the future role of guards on trains. Members of the National Un… [+1418 chars]In an attempt to boost diversity and inclusion efforts and civic engagement between the growing technology industry in Los Angeles and the community that surrounds it, over 80 venture capitalists and entrepreneurs joined the city’s mayor, Eric Garcetti, and t… [+3672 chars]Image copyright PA Theresa May has insisted it is the UK\\'s national interest at stake over Brexit, not her own \"personal fortunes\". The prime minister told the Sun that ending the deadlock in the negotiations was \"not about her\" but about ensuring the best po… [+4207 chars]Image copyright AFP/Getty Image caption Mr Kanu said that Israel\\'s spy agency Mossad have been helping him Missing Nigerian separatist leader Nnamdi Kanu has resurfaced in Israel one year after soldiers stormed his home in the southern Abia state. \"I\\'m in Isr… [+2001 chars]The latest five minute news bulletin from BBC World Service.Manchester United manager Jose Mourinho jokes about answering a question in Portuguese before Tuesday\\'s Champions League game against Juventus. The Portuguese was charged last week by the Football Association over comments he made to a television camera after… [+100 chars]Only three of Wigan\\'s 20 points so far this season have come away from home Millwall could give starts to defender Shaun Hutchinson and striker Tom Bradshaw against Wigan on Tuesday. Hutchinson has not played for the Lions since 22 August because of a knee in… [+1071 chars]',\n",
       " 'Media caption An eyewitness describes hearing shouting in the \"disorientating\" thick fog A ferry operator is to change its procedures for sailing in foggy conditions after a car ferry hit several yachts and ran aground off the Isle of Wight. The Red Falcon go… [+1750 chars]Image copyright Riverford Organic Farmers Image caption Guy Singh-Watson\\'s company supplies organic vegetables to tens of thousands of homes each week. The anti-plastic \"fervour\" sweeping across the UK is weakening the fight against climate change, the founde… [+3955 chars]',\n",
       " 'Image copyright RMT Image caption Further 24-hour strikes will take place on SWR on consecutive Saturdays from 3-24 November Staff on South Western Railway (SWR) have begun a five-day strike over the future role of guards on trains. Members of the National Un… [+1418 chars]In an attempt to boost diversity and inclusion efforts and civic engagement between the growing technology industry in Los Angeles and the community that surrounds it, over 80 venture capitalists and entrepreneurs joined the city’s mayor, Eric Garcetti, and t… [+3672 chars]Image copyright PA Theresa May has insisted it is the UK\\'s national interest at stake over Brexit, not her own \"personal fortunes\". The prime minister told the Sun that ending the deadlock in the negotiations was \"not about her\" but about ensuring the best po… [+4207 chars]Image copyright AFP/Getty Image caption Mr Kanu said that Israel\\'s spy agency Mossad have been helping him Missing Nigerian separatist leader Nnamdi Kanu has resurfaced in Israel one year after soldiers stormed his home in the southern Abia state. \"I\\'m in Isr… [+2001 chars]The latest five minute news bulletin from BBC World Service.Manchester United manager Jose Mourinho jokes about answering a question in Portuguese before Tuesday\\'s Champions League game against Juventus. The Portuguese was charged last week by the Football Association over comments he made to a television camera after… [+100 chars]Only three of Wigan\\'s 20 points so far this season have come away from home Millwall could give starts to defender Shaun Hutchinson and striker Tom Bradshaw against Wigan on Tuesday. Hutchinson has not played for the Lions since 22 August because of a knee in… [+1071 chars]',\n",
       " 'Media caption What happens during a smear test? About three million women across England have not had a smear test for at least three-and-a-half years. GPs are trying to improve take-up rates as figures show up to half of women under 50 in some areas have not… [+6197 chars]Image copyright PA The reintroduction of beavers in parts of Scotland is a positive example of \"rewilding\", according to a new study. Prof Nigel Willby has written about the re-establishment of populations of beavers for the Royal Society of Biological Scienc… [+2307 chars]Porters at Aberdeen Royal Infirmary are being given screening for tuberculosis (TB) after one of the team was diagnosed with the respiratory disease. Tests have also been offered to those living in the same houses as the porters. However, NHS Grampian said th… [+155 chars]']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# idf (*inverse document frequency*)\n",
    "\n",
    "Trabajar unicamente con las frecuencias de los términos conlleva un problema: todos los términos presentes en la colección se consideran igualmente relevantes a la hora de discriminar la relevancia de los documentos, atendiendo a sus frecuencias. Y resulta que esto no es verdad. \n",
    "\n",
    "Imaginemos un corpus en el que la frecuencia total de dos términos concretos, *este* y *fonema*, es similar en términos absolutos. La distribución de estos términos a lo largo de la coleccion es seguramente muy diferente. El primero aparece con una distribución uniforme a lo largo del corpus, su capacidad discriminativa es baja y debería penalizarse a la hora de asignar relevancia (como el resto de *stopwords*). El segundo, por el contrario, se concentra principalmente en documentos que hablan de fonología, su capacidad discriminativa es alta y debería ser premiado.\n",
    "\n",
    "Existen mecanismos correctores para incorporar estas penalizaciones y premios en nuestros pesos. Los más habituales pasan por recurrir a la frecuencia de documento $\\mbox{df}_t$, definida como el número de documentos de la colección $D$ que contienen el término $t$: $\\mbox{df}_t = {|\\{d \\in D: t \\in d\\}|}$.\n",
    "\n",
    "Más concretamente, se calcula la frecuencia inversa de documento, o [**idf**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency_2) (*inverse document frequency*), definida como: $\\mbox{idf}_t = \\log {|D|\\over \\mbox{df}_t}$, donde $|D|$ indica el número total de documentos de nuestra colección. De este modo, el **idf** de un término específico pero muy discriminativo será alto, mientras que el de un término muy frecuente a lo largo de la coleccion será bajo.\n",
    "\n",
    "## Calculando df\n",
    "\n",
    "### A olalla dice que me quede solo cos substantivos, adxetivos e verbos, porque senon os artículos, preposicións ensucian muito o resultado en textos tan pequenos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('was', 4),\n",
       " ('in', 4),\n",
       " ('A', 4),\n",
       " ('on', 4),\n",
       " ('the', 4),\n",
       " ('with', 4),\n",
       " ('a', 4),\n",
       " ('after', 4),\n",
       " ('at', 4),\n",
       " ('to', 4)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "df={}\n",
    "\n",
    "for word in tf:\n",
    "    df[word]=0\n",
    "    for text in corpus:\n",
    "        if word in text:\n",
    "            df[word]+=1\n",
    "\n",
    "#Ordenamos os valores de df polo value e quedamosns cos 20 primeriso\n",
    "m=sorted(df.items(),key=operator.itemgetter(1),reverse=True)\n",
    "m[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores de **df** son números enteros: el número de documentos del corpus que contienen cada uno de los términos.\n",
    "\n",
    "## Calculando idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "idf={}\n",
    "for word in df:\n",
    "    idf[word]= math.log(len(corpus)/df[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.idf\n",
    "\n",
    "[**td.idf**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (*term frequency - inverse document frequency*) es una medida numérica que expresa la relevancia de una palabra de un documento con respecto a una colección de documentos. Es uno de los esquemas de pesado más comunes en las tareas relacionadas con la recuperación de información y la minería de texto.\n",
    "\n",
    "El objetivo de esta métrica es representar los documentos de texto como vectores, ignorando el orden concreto de las palabras pero manteniendo la información relativa a las frecuencias de aparición. \n",
    "\n",
    "El valor de tf-idf de una palabra:\n",
    "\n",
    "- es mayor cuanto más frecuente sea esta palabra dentro de un documento concreto, pero;\n",
    "- es mayor cuando menos común sea la palabra en otros documentos de la colección.\n",
    "\n",
    "Estas dos características premian a los términos que son muy frecuentes en determinados documentos concretos pero poco comunes en general: estos términos pueden considerarse buenos descriptores de un conjunto de documentos. Y a la vez, penalizan aquellos términos que aparecen con mucha frecuencia a lo largo de toda la colección, como las *stopwords*.\n",
    "\n",
    "\n",
    "## Calculando **tf.idf**\n",
    "\n",
    "**tf.idf** se calcula como el producto de dos términos: $\\mathrm{tf.idf}(t, d, D) = \\mathrm{tf}(t, d) \\times \\mathrm{idf}(t, D)$\n",
    "\n",
    "- la frecuencia de un término (tf): el número de veces que una determinada palabra aparece en un documento. \n",
    "\n",
    "- la frecuencia inversa de documento (idf): el logaritmo del número total de documentos en el corpus dividido entre el número de documentos en los que el término aparece.\n",
    "\n",
    "Ya hemos calculado previamente esos valores. Bastará con realizar los productos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos o tfidf\n",
    "tfidf = {}\n",
    "\n",
    "for word in tf:\n",
    "    tfidf[word]=[]\n",
    "    for value in tf[word]:\n",
    "        tfidf[word].append(value*idf[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first corpus to be sorted are the words from the business\n",
    "#The words from the business corpus are sorted by its value (tfidf).      \n",
    "tfidf_business=sorted(tfidf.items(),key= lambda x : x[1][0],reverse=True)[:50]\n",
    "#The second corpus to be sorted are the words from the science/technology corpus.\n",
    "#The words from the science/technology corpus are sorted by its value (tfidf)      \n",
    "tfidf_sciencetechnology=sorted(tfidf.items(),key= lambda x : x[1][1],reverse=True)[:50]\n",
    "#The third corpus to be sorted are the words from the entertainment corpus\n",
    "#The words from the entertainment corpus are sorted by its value (tfidf)     \n",
    "tfidf_entertainment=sorted(tfidf.items(),key= lambda x : x[1][2],reverse=True)[:50]\n",
    "#The fourth corpus to be sorted are the words from the health corpus.\n",
    "#The words from the health corpus are sorted by its value (tfidf)      \n",
    "tfidf_health =sorted(tfidf.items(),key= lambda x : x[1][3],reverse=True)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the data presented in the following tables.\n",
    "\n",
    "The higher of the **tfidf** value in a specific corpus, the more importance of the word in that corpus.<br>\n",
    "Note that a word is relevant in a corpus, when the word is repeated a lot of times in the corpus itself, and it is not present in other corpus.\n",
    "Therefore, the different tables show the most important words for the business, entertainment, science/technology and\n",
    "health corpus. Bear in mind that these words have a high value just in one of the corpus, and for the rest of them, the value is low or even 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_Words</th>\n",
       "      <th>1_Business</th>\n",
       "      <th>2_science&amp;technology</th>\n",
       "      <th>3_entertainment</th>\n",
       "      <th>4_health</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BBC</td>\n",
       "      <td>0.010294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010294</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>his</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>died</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>near</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Golf</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chars]The</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Image</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>copyright</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>has</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>from</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.002557</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>caption</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Police</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Scotland/Google</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Stephen</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Wallace</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>found</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dead</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Williamsburgh</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Court,</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Paisley</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0_Words  1_Business  2_science&technology  3_entertainment  \\\n",
       "0               BBC    0.010294              0.000000         0.010294   \n",
       "1               his    0.006863              0.000000         0.006863   \n",
       "2              died    0.006863              0.000000         0.006863   \n",
       "3              near    0.006863              0.000000         0.006863   \n",
       "4              Golf    0.006863              0.000000         0.006863   \n",
       "5         chars]The    0.006863              0.000000         0.006863   \n",
       "6             Image    0.005697              0.005114         0.005697   \n",
       "7         copyright    0.005697              0.003836         0.005697   \n",
       "8               has    0.005697              0.003836         0.005697   \n",
       "9              from    0.005697              0.002557         0.005697   \n",
       "10          caption    0.004273              0.003836         0.004273   \n",
       "11           Police    0.003431              0.000000         0.003431   \n",
       "12  Scotland/Google    0.003431              0.000000         0.003431   \n",
       "13          Stephen    0.003431              0.000000         0.003431   \n",
       "14          Wallace    0.003431              0.000000         0.003431   \n",
       "15            found    0.003431              0.000000         0.003431   \n",
       "16             dead    0.003431              0.000000         0.003431   \n",
       "17    Williamsburgh    0.003431              0.000000         0.003431   \n",
       "18           Court,    0.003431              0.000000         0.003431   \n",
       "19          Paisley    0.003431              0.000000         0.003431   \n",
       "\n",
       "    4_health  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        0.0  \n",
       "5        0.0  \n",
       "6        0.0  \n",
       "7        0.0  \n",
       "8        0.0  \n",
       "9        0.0  \n",
       "10       0.0  \n",
       "11       0.0  \n",
       "12       0.0  \n",
       "13       0.0  \n",
       "14       0.0  \n",
       "15       0.0  \n",
       "16       0.0  \n",
       "17       0.0  \n",
       "18       0.0  \n",
       "19       0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The top 10 most important words of the business corpus are represented in a table.\n",
    "n=10\n",
    "y_words_b=[tfidf_business[y][0] for y in range(n)]\n",
    "y_business=[tfidf_business[y][1][0] for y in range(n) ]\n",
    "y_business_sciencetechnolgy=[tfidf_business[y][1][1] for y in range(n) ]\n",
    "y_business_entertainment=[tfidf_business[y][1][2] for y in range(n) ]\n",
    "y_business_health=[tfidf_business[y][1][3] for y in range(n) ]\n",
    "\n",
    "pd.DataFrame({'0_Words':y_words_b,'1_Business':y_business, '2_science&technology':y_business_sciencetechnolgy,\n",
    "             '3_entertainment':y_business_entertainment, '4_health':y_business_health})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_Words</th>\n",
       "      <th>1_Business</th>\n",
       "      <th>2_science&amp;technology</th>\n",
       "      <th>3_entertainment</th>\n",
       "      <th>4_health</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>report</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decentralized</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>social</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tractor</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>its</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>load</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>potatoes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>first-class</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Getty</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Images</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Demonstrators</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>York</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>protested</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sunday</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>evening</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>outpouring</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>anger</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>politicians,</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>celebrities</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0_Words  1_Business  2_science&technology  3_entertainment  4_health\n",
       "0          report         0.0              0.012323              0.0  0.000000\n",
       "1   decentralized         0.0              0.012323              0.0  0.000000\n",
       "2          social         0.0              0.012323              0.0  0.000000\n",
       "3         tractor         0.0              0.012323              0.0  0.000000\n",
       "4             its         0.0              0.012323              0.0  0.007967\n",
       "5            load         0.0              0.012323              0.0  0.000000\n",
       "6        potatoes         0.0              0.012323              0.0  0.000000\n",
       "7     first-class         0.0              0.012323              0.0  0.000000\n",
       "8           Getty         0.0              0.006161              0.0  0.000000\n",
       "9          Images         0.0              0.006161              0.0  0.000000\n",
       "10  Demonstrators         0.0              0.006161              0.0  0.000000\n",
       "11           York         0.0              0.006161              0.0  0.000000\n",
       "12      protested         0.0              0.006161              0.0  0.000000\n",
       "13         Sunday         0.0              0.006161              0.0  0.000000\n",
       "14        evening         0.0              0.006161              0.0  0.000000\n",
       "15     outpouring         0.0              0.006161              0.0  0.000000\n",
       "16          anger         0.0              0.006161              0.0  0.000000\n",
       "17             US         0.0              0.006161              0.0  0.015934\n",
       "18   politicians,         0.0              0.006161              0.0  0.000000\n",
       "19    celebrities         0.0              0.006161              0.0  0.000000"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The top 10 most important words from the science/technolgy corpus are showed in the below table.\n",
    "\n",
    "y_words_sc=[tfidf_sciencetechnology[y][0] for y in range(n)]\n",
    "y_sciencetechnolgy_business=[tfidf_sciencetechnology[y][1][0] for y in range(n) ]\n",
    "y_sciencetechnolgy=[tfidf_sciencetechnology[y][1][1] for y in range(n) ]\n",
    "y_sciencetechnolgy_entertainment=[tfidf_sciencetechnology[y][1][2] for y in range(n) ]\n",
    "y_sciencetechnolgy_health=[tfidf_sciencetechnology[y][1][3] for y in range(n)]\n",
    "\n",
    "pd.DataFrame({'0_Words':y_words_sc,'1_Business':y_sciencetechnolgy_business, '2_science&technology':y_sciencetechnolgy,\n",
    "             '3_entertainment':y_sciencetechnolgy_entertainment, '4_health':y_sciencetechnolgy_health})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_Words</th>\n",
       "      <th>1_Business</th>\n",
       "      <th>2_science&amp;technology</th>\n",
       "      <th>3_entertainment</th>\n",
       "      <th>4_health</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BBC</td>\n",
       "      <td>0.010294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010294</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>his</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>died</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>near</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Golf</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chars]The</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Image</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>copyright</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>has</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>from</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.002557</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>caption</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Police</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Scotland/Google</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Stephen</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Wallace</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>found</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dead</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Williamsburgh</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Court,</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Paisley</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0_Words  1_Business  2_science&technology  3_entertainment  \\\n",
       "0               BBC    0.010294              0.000000         0.010294   \n",
       "1               his    0.006863              0.000000         0.006863   \n",
       "2              died    0.006863              0.000000         0.006863   \n",
       "3              near    0.006863              0.000000         0.006863   \n",
       "4              Golf    0.006863              0.000000         0.006863   \n",
       "5         chars]The    0.006863              0.000000         0.006863   \n",
       "6             Image    0.005697              0.005114         0.005697   \n",
       "7         copyright    0.005697              0.003836         0.005697   \n",
       "8               has    0.005697              0.003836         0.005697   \n",
       "9              from    0.005697              0.002557         0.005697   \n",
       "10          caption    0.004273              0.003836         0.004273   \n",
       "11           Police    0.003431              0.000000         0.003431   \n",
       "12  Scotland/Google    0.003431              0.000000         0.003431   \n",
       "13          Stephen    0.003431              0.000000         0.003431   \n",
       "14          Wallace    0.003431              0.000000         0.003431   \n",
       "15            found    0.003431              0.000000         0.003431   \n",
       "16             dead    0.003431              0.000000         0.003431   \n",
       "17    Williamsburgh    0.003431              0.000000         0.003431   \n",
       "18           Court,    0.003431              0.000000         0.003431   \n",
       "19          Paisley    0.003431              0.000000         0.003431   \n",
       "\n",
       "    4_health  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        0.0  \n",
       "5        0.0  \n",
       "6        0.0  \n",
       "7        0.0  \n",
       "8        0.0  \n",
       "9        0.0  \n",
       "10       0.0  \n",
       "11       0.0  \n",
       "12       0.0  \n",
       "13       0.0  \n",
       "14       0.0  \n",
       "15       0.0  \n",
       "16       0.0  \n",
       "17       0.0  \n",
       "18       0.0  \n",
       "19       0.0  "
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The top 10 most important words from the entertainment corpus are showed in the below table.\n",
    "\n",
    "y_words_e=[tfidf_entertainment[y][0] for y in range(n)]\n",
    "y_entertainment_business=[tfidf_entertainment[y][1][0] for y in range(n) ]\n",
    "y_entertainment_sciencetechnolgy=[tfidf_entertainment[y][1][1] for y in range(n) ]\n",
    "y_entertainment=[tfidf_entertainment[y][1][2] for y in range(n) ]\n",
    "y_entertainment_health=[tfidf_entertainment[y][1][3] for y in range(n) ]\n",
    "\n",
    "pd.DataFrame({'0_Words':y_words_e,'1_Business':y_entertainment_business, '2_science&technology':y_entertainment_sciencetechnolgy,\n",
    "             '3_entertainment':y_entertainment, '4_health':y_entertainment_health})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_Words</th>\n",
       "      <th>1_Business</th>\n",
       "      <th>2_science&amp;technology</th>\n",
       "      <th>3_entertainment</th>\n",
       "      <th>4_health</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saudi</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bradbury</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>over</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Arabia</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>have</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>made</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>big</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>effort</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>improve</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>image;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Italy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>clashes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>EU</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>budget</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>proposals;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Palestinian</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>music</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Le</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0_Words  1_Business  2_science&technology  3_entertainment  4_health\n",
       "0         Saudi         0.0              0.000000              0.0  0.031869\n",
       "1      Bradbury         0.0              0.000000              0.0  0.031869\n",
       "2            US         0.0              0.006161              0.0  0.015934\n",
       "3          over         0.0              0.006161              0.0  0.015934\n",
       "4           How         0.0              0.000000              0.0  0.015934\n",
       "5        Arabia         0.0              0.000000              0.0  0.015934\n",
       "6          have         0.0              0.000000              0.0  0.015934\n",
       "7          made         0.0              0.000000              0.0  0.015934\n",
       "8           big         0.0              0.000000              0.0  0.015934\n",
       "9        effort         0.0              0.000000              0.0  0.015934\n",
       "10      improve         0.0              0.000000              0.0  0.015934\n",
       "11       image;         0.0              0.000000              0.0  0.015934\n",
       "12        Italy         0.0              0.000000              0.0  0.015934\n",
       "13      clashes         0.0              0.000000              0.0  0.015934\n",
       "14           EU         0.0              0.000000              0.0  0.015934\n",
       "15       budget         0.0              0.000000              0.0  0.015934\n",
       "16   proposals;         0.0              0.000000              0.0  0.015934\n",
       "17  Palestinian         0.0              0.000000              0.0  0.015934\n",
       "18        music         0.0              0.000000              0.0  0.015934\n",
       "19           Le         0.0              0.000000              0.0  0.015934"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The top 10 most important words from the health corpus are showed in the below table.\n",
    "\n",
    "y_words_h=[tfidf_health[y][0] for y in range(n)]\n",
    "y_health_business=[tfidf_health[y][1][0] for y in range(n) ]\n",
    "y_health_sciencetechnolgy=[tfidf_health[y][1][1] for y in range(n) ]\n",
    "y_health_entertainment=[tfidf_health[y][1][2] for y in range(n) ]\n",
    "y_health=[tfidf_health[y][1][3] for y in range(n) ]\n",
    "\n",
    "pd.DataFrame({'0_Words':y_words_h,'1_Business':y_health_business, '2_science&technology':y_health_sciencetechnolgy,\n",
    "             '3_entertainment':y_health_entertainment, '4_health':y_health})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
