{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __**BLOCK 2**__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Using the model and analyzing the most frequent words in a set of news obtained from a media organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get the set of news, the API news is going to be used. This API is available for Python and collects news depending on the media organization, date, language...\n",
    "#### For more information related to this API see the below link:\n",
    "#### https://newsapi.org/docs/get-started\n",
    "##### Note: An internet connection is required to run the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The code of this notebook was used to create the Dash interface part 2. Note that the analysis related to the most relevant words of each corpus, was not included in the Dash interface. See the \"5_Dash_framework_part_2.ipynb\" notebook for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use the API the below package should be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install newsapi-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data used to train the model\n",
    "path_file= './data/dataset_news.csv'\n",
    "file=pd.read_csv(path_file, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function is created to normalize the text, removing punctuation symbols and double spaces. This cleaned data is \n",
    "#saved in a new column ('TEXT').\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove punctuation that is not word-internal (e.g., hyphens, apostrophes)\n",
    "    text = re.sub('\\s\\W',' ',text)\n",
    "    text = re.sub('\\W\\s',' ',text)\n",
    "    \n",
    "    # make sure we didn't introduce any double spaces\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "file['TEXT'] = [normalize_text(text) for text in file['TITLE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The text is going to be vectorized.  For more info see the below link:\n",
    "#http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(file['TEXT'])\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(file['CATEGORY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the Naive Bayes model e dividimos o data set co StrtifiedKfold polas razóns da explicación de arriba\n",
    "### the data is splitted using the Strtified K fold.\n",
    "kf = StratifiedKFold(n_splits=10,shuffle=True,random_state=42)\n",
    "i=1\n",
    "score_mean=[]\n",
    "\n",
    "for train_index,test_index in kf.split(X,y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    #Naive Bayes model\n",
    "    nb = MultinomialNB()\n",
    "    classifier_model = nb.fit(X_train, y_train)\n",
    "    score = nb.score(X_test, y_test)\n",
    "    score_mean.append(score)\n",
    "    print(score)\n",
    "    i+=1\n",
    "print('The mean of the whole predictions is:',sum(score_mean)/len(score_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Getting the news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With this API, it is possible to get news from different media sources. \n",
    "### For more information see the below link:\n",
    "https://newsapi.org/docs/get-started\n",
    "\n",
    "##### Note: An internet connection is required to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The api_key is the key from my account, which can be used to obtain the set of news.\n",
    "newsapi = NewsApiClient(api_key='9fe0d6dd387c40bc8cb5fdec346f0bda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today= str(date.today())\n",
    "day_before_yesterday = str(date.today()-timedelta(2))\n",
    "\n",
    "#For more information related to the newspi.get_everything, see te link https://newsapi.org/docs/endpoints/everything\n",
    "\n",
    "all_articles = newsapi.get_everything(sources='bbc-news',\n",
    "                                      from_param= day_before_yesterday,\n",
    "                                      to= today,\n",
    "                                      language='en',\n",
    "                                      sort_by='relevancy',\n",
    "                                      page=5)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To see the available media sources\n",
    "sources = newsapi.get_sources()\n",
    "sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Missing values in the data set obtained from the API news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the news are downloaded from the API, they contain\n",
    "none values in the value of the ['content'] key. This none values will cause problems when the news are classified \n",
    "with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are going to select just the key 'content' which has text. When the news are downloaded from the API, they contain\n",
    "#none values in the value of the ['content'] key. This none values will cause problemns when the news are classified \n",
    "#with our model\n",
    "clean_text=[]\n",
    "for text in all_articles['articles']:\n",
    "    #if the value of the ['content']key is a string, it is putted in a new list (clean_text).\n",
    "    if type(text['content'])==str:\n",
    "        clean_text.append(text)  \n",
    "clean_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Plotting the classification of the news performed by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([\"business\", \"science and technology\", \"entertainment\", \"health\"])\n",
    "\n",
    "x_axis=[]\n",
    "\n",
    "corpus_business=''\n",
    "corpus_sciencetechnology=''\n",
    "corpus_entertainment=''\n",
    "corpus_health=''\n",
    "\n",
    "#The classification of the news is performed. Thus, each news is placed in its proper corpus.\n",
    "#In case there are News with a None in their content, the code will not run.\n",
    "for i in range(len(clean_text)):\n",
    "    prediction = nb.predict(vectorizer.transform([clean_text[i]['content']]))\n",
    "    x_axis.append(list(le.inverse_transform(prediction)))\n",
    "    #Each news is placed in its corpus.\n",
    "    if list(le.inverse_transform(prediction)) == ['business']:\n",
    "        corpus_business= corpus_business + '' + clean_text[i]['content']\n",
    "    if list(le.inverse_transform(prediction)) == ['science and technology']:\n",
    "        corpus_sciencetechnology= corpus_sciencetechnology + '' + clean_text[i]['content']\n",
    "    if list(le.inverse_transform(prediction)) == ['entertainment']:\n",
    "        corpus_entertainment= corpus_entertainment + '' + clean_text[i]['content']\n",
    "    if list(le.inverse_transform(prediction)) == ['health']:\n",
    "        corpus_health= corpus_health + '' + clean_text[i]['content']\n",
    "\n",
    "corpus=[corpus_business,corpus_sciencetechnology,corpus_entertainment,corpus_health]\n",
    "\n",
    "#The results will be putted just in one list. Without lists in the middle.\n",
    "x_axis_list=[]\n",
    "for i in range(len(x_axis)):\n",
    "    x_axis_list.append(x_axis[i][0])\n",
    "#The number of times that each element appears in the list is calculated.\n",
    "x=Counter(x_axis_list)\n",
    "\n",
    "#A bar_graph is plotted in order to see the number of news of each topic.\n",
    "labels, values = zip(*Counter(x_axis_list).items())\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "plt.bar(indexes, values)\n",
    "plt.xticks(indexes , labels, rotation= 'vertical')\n",
    "plt.title('Number of news for each topic')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Calculating the most relevant words of each class (business, entertainment, science/technology and health)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1 tf (*term frequency*)\n",
    "\n",
    "[**tf**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Term_frequency_2) es el peso que indica la frecuencia de un término, es decir, el número de veces que una determinada palabra aparece en un documento. \n",
    "\n",
    "La aproximación más sencilla consiste consiste en asignar como peso para el término $t$ en el documento $d$ del corpus $D$ (denotado como $\\mbox{tf}_{t,d}$) el número de ocurrencias de $t$ en $d$. Es recomendable normalizar esta frecuencia, diviendo el número de ocurrencias entre el número total de palabras de un documento, para no penalizar los documentos breves: $\\mathrm{tf}(t,d) = \\frac{\\mathrm{f}(t, d)}{\\max\\{\\mathrm{f}(w, d):w \\in d\\}}$\n",
    "\n",
    "Vamos a calcularlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf={}\n",
    "i=-1\n",
    "for text in corpus:\n",
    "    sentence=text.split()\n",
    "    for word in sentence:\n",
    "        tf[word]=[0]*len(corpus)\n",
    "\n",
    "for text in corpus:\n",
    "    sentence=text.split()\n",
    "    i= i+1\n",
    "    for word in sentence:\n",
    "        tf[word][i]=sentence.count(word)/len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.2 idf (*inverse document frequency*)\n",
    "\n",
    "Trabajar unicamente con las frecuencias de los términos conlleva un problema: todos los términos presentes en la colección se consideran igualmente relevantes a la hora de discriminar la relevancia de los documentos, atendiendo a sus frecuencias. Y resulta que esto no es verdad. \n",
    "\n",
    "Imaginemos un corpus en el que la frecuencia total de dos términos concretos, *este* y *fonema*, es similar en términos absolutos. La distribución de estos términos a lo largo de la coleccion es seguramente muy diferente. El primero aparece con una distribución uniforme a lo largo del corpus, su capacidad discriminativa es baja y debería penalizarse a la hora de asignar relevancia (como el resto de *stopwords*). El segundo, por el contrario, se concentra principalmente en documentos que hablan de fonología, su capacidad discriminativa es alta y debería ser premiado.\n",
    "\n",
    "Existen mecanismos correctores para incorporar estas penalizaciones y premios en nuestros pesos. Los más habituales pasan por recurrir a la frecuencia de documento $\\mbox{df}_t$, definida como el número de documentos de la colección $D$ que contienen el término $t$: $\\mbox{df}_t = {|\\{d \\in D: t \\in d\\}|}$.\n",
    "\n",
    "Más concretamente, se calcula la frecuencia inversa de documento, o [**idf**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency_2) (*inverse document frequency*), definida como: $\\mbox{idf}_t = \\log {|D|\\over \\mbox{df}_t}$, donde $|D|$ indica el número total de documentos de nuestra colección. De este modo, el **idf** de un término específico pero muy discriminativo será alto, mientras que el de un término muy frecuente a lo largo de la coleccion será bajo.\n",
    "\n",
    "##### Calculando df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "df={}\n",
    "\n",
    "for word in tf:\n",
    "    df[word]=0\n",
    "    for text in corpus:\n",
    "        if word in text:\n",
    "            df[word]+=1\n",
    "\n",
    "#Ordenamos os valores de df polo value e quedamosns cos 20 primeriso\n",
    "m=sorted(df.items(),key=operator.itemgetter(1),reverse=True)\n",
    "m[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores de **df** son números enteros: el número de documentos del corpus que contienen cada uno de los términos.\n",
    "\n",
    "##### Calculando idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "idf={}\n",
    "for word in df:\n",
    "    idf[word]= math.log(len(corpus)/df[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.3 tf.idf\n",
    "\n",
    "[**td.idf**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (*term frequency - inverse document frequency*) es una medida numérica que expresa la relevancia de una palabra de un documento con respecto a una colección de documentos. Es uno de los esquemas de pesado más comunes en las tareas relacionadas con la recuperación de información y la minería de texto.\n",
    "\n",
    "El objetivo de esta métrica es representar los documentos de texto como vectores, ignorando el orden concreto de las palabras pero manteniendo la información relativa a las frecuencias de aparición. \n",
    "\n",
    "El valor de tf-idf de una palabra:\n",
    "\n",
    "- es mayor cuanto más frecuente sea esta palabra dentro de un documento concreto, pero;\n",
    "- es mayor cuando menos común sea la palabra en otros documentos de la colección.\n",
    "\n",
    "Estas dos características premian a los términos que son muy frecuentes en determinados documentos concretos pero poco comunes en general: estos términos pueden considerarse buenos descriptores de un conjunto de documentos. Y a la vez, penalizan aquellos términos que aparecen con mucha frecuencia a lo largo de toda la colección, como las *stopwords*.\n",
    "\n",
    "\n",
    "#### Calculando **tf.idf**\n",
    "\n",
    "**tf.idf** se calcula como el producto de dos términos: $\\mathrm{tf.idf}(t, d, D) = \\mathrm{tf}(t, d) \\times \\mathrm{idf}(t, D)$\n",
    "\n",
    "- la frecuencia de un término (tf): el número de veces que una determinada palabra aparece en un documento. \n",
    "\n",
    "- la frecuencia inversa de documento (idf): el logaritmo del número total de documentos en el corpus dividido entre el número de documentos en los que el término aparece.\n",
    "\n",
    "Ya hemos calculado previamente esos valores. Bastará con realizar los productos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos o tfidf\n",
    "tfidf = {}\n",
    "\n",
    "for word in tf:\n",
    "    tfidf[word]=[]\n",
    "    for value in tf[word]:\n",
    "        tfidf[word].append(value*idf[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first corpus to be sorted are the words from the business\n",
    "#The words from the business corpus are sorted by its value (tfidf).      \n",
    "tfidf_business=sorted(tfidf.items(),key= lambda x : x[1][0],reverse=True)\n",
    "#The second corpus to be sorted are the words from the science/technology corpus.\n",
    "#The words from the science/technology corpus are sorted by its value (tfidf)      \n",
    "tfidf_sciencetechnology=sorted(tfidf.items(),key= lambda x : x[1][1],reverse=True)\n",
    "#The third corpus to be sorted are the words from the entertainment corpus\n",
    "#The words from the entertainment corpus are sorted by its value (tfidf)     \n",
    "tfidf_entertainment=sorted(tfidf.items(),key= lambda x : x[1][2],reverse=True)\n",
    "#The fourth corpus to be sorted are the words from the health corpus.\n",
    "#The words from the health corpus are sorted by its value (tfidf)      \n",
    "tfidf_health =sorted(tfidf.items(),key= lambda x : x[1][3],reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.4 Representing the most important word of each corpus in a table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the data presented in the following tables.\n",
    "\n",
    "The higher of the **tfidf** value in a specific corpus, the more importance of the word in that corpus.<br>\n",
    "Note that a word is relevant in a corpus, when the word is repeated a lot of times in the corpus itself, and it is not present in other corpus.\n",
    "Therefore, the different tables show the most important words for the business, entertainment, science/technology and\n",
    "health corpus. Bear in mind that these words have a high value just in one of the corpus, and for the rest of them, the value is low or even 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The top 10 most important words of the business corpus are represented in a table.\n",
    "\n",
    "#When the tf_idf_business is 0 (there are no business news nor words to show ), the code is not execute.\n",
    "if tfidf_business[1][1][0]!=0:\n",
    "    n=10\n",
    "    y_words_b=[tfidf_business[y][0] for y in range(n)]\n",
    "    y_business=[tfidf_business[y][1][0] for y in range(n) ]\n",
    "    y_business_sciencetechnolgy=[tfidf_business[y][1][1] for y in range(n) ]\n",
    "    y_business_entertainment=[tfidf_business[y][1][2] for y in range(n) ]\n",
    "    y_business_health=[tfidf_business[y][1][3] for y in range(n) ]\n",
    "\n",
    "    print('\\033[1mThe most important words of the business corpus')\n",
    "\n",
    "    display(pd.DataFrame({'0_Words':y_words_b,'1_Business':y_business, '2_science&technology':y_business_sciencetechnolgy,\n",
    "                 '3_entertainment':y_business_entertainment, '4_health':y_business_health}))\n",
    "else:\n",
    "    print('\\033[1mThere are no news classified as business type, therefore there are not words to show')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The top 10 most important words from the science/technolgy corpus are showed in the below table.\n",
    "\n",
    "#When the tf_idf_science/technolgy is 0 (there are no science/technolgy news nor words to show ), the code is not execute.\n",
    "if tfidf_sciencetechnology[1][1][1]!=0:\n",
    "    n=10\n",
    "    y_words_sc=[tfidf_sciencetechnology[y][0] for y in range(n)]\n",
    "    y_sciencetechnolgy_business=[tfidf_sciencetechnology[y][1][0] for y in range(n) ]\n",
    "    y_sciencetechnolgy=[tfidf_sciencetechnology[y][1][1] for y in range(n) ]\n",
    "    y_sciencetechnolgy_entertainment=[tfidf_sciencetechnology[y][1][2] for y in range(n) ]\n",
    "    y_sciencetechnolgy_health=[tfidf_sciencetechnology[y][1][3] for y in range(n)]\n",
    "\n",
    "    print('\\033[1mThe most important words of the science and technolgy corpus')\n",
    "\n",
    "    display(pd.DataFrame({'0_Words':y_words_sc,'1_Business':y_sciencetechnolgy_business, '2_science&technology':y_sciencetechnolgy,\n",
    "             '3_entertainment':y_sciencetechnolgy_entertainment, '4_health':y_sciencetechnolgy_health}))\n",
    "else:\n",
    "    print('\\033[1mThere are no news classified as science & technology type, therefore there are not words to show')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The top 10 most important words from the entertainment corpus are showed in the below table.\n",
    "#When the tf_idf_entertainment is 0 (there are no entertainment news nor words to show ), the code is not execute.\n",
    "if tfidf_entertainment[1][1][2]!=0:\n",
    "    y_words_e=[tfidf_entertainment[y][0] for y in range(n)]\n",
    "    y_entertainment_business=[tfidf_entertainment[y][1][0] for y in range(n) ]\n",
    "    y_entertainment_sciencetechnolgy=[tfidf_entertainment[y][1][1] for y in range(n) ]\n",
    "    y_entertainment=[tfidf_entertainment[y][1][2] for y in range(n) ]\n",
    "    y_entertainment_health=[tfidf_entertainment[y][1][3] for y in range(n) ]\n",
    "\n",
    "    print('\\033[1mThe most important words of the entertainment corpus')\n",
    "\n",
    "    display(pd.DataFrame({'0_Words':y_words_e,'1_Business':y_entertainment_business, '2_science&technology':y_entertainment_sciencetechnolgy,\n",
    "                 '3_entertainment':y_entertainment, '4_health':y_entertainment_health}))\n",
    "else:\n",
    "    print('\\033[1mThere are no news classified as entertainment type, therefore there are not words to show')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The top 10 most important words from the health corpus are showed in the below table.\n",
    "#When the tf_idf_health is 0 (there are no entertainment news nor words to show ), the code is not execute.\n",
    "if tfidf_health[1][1][3]!=0:\n",
    "    y_words_h=[tfidf_health[y][0] for y in range(n)]\n",
    "    y_health_business=[tfidf_health[y][1][0] for y in range(n) ]\n",
    "    y_health_sciencetechnolgy=[tfidf_health[y][1][1] for y in range(n) ]\n",
    "    y_health_entertainment=[tfidf_health[y][1][2] for y in range(n) ]\n",
    "    y_health=[tfidf_health[y][1][3] for y in range(n) ]\n",
    "\n",
    "    print('\\033[1mThe most important words of the health corpus')\n",
    "\n",
    "    display(pd.DataFrame({'0_Words':y_words_h,'1_Business':y_health_business, '2_science&technology':y_health_sciencetechnolgy,\n",
    "                 '3_entertainment':y_health_entertainment, '4_health':y_health}))\n",
    "else:\n",
    "    print('\\033[1mThere are no news classified as health type, therefore there are not words to show')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: As the number of news is too low, the representative words of each corpus are not the typical values expected by a person. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
