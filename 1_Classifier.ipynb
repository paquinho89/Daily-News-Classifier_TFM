{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __**BLOCK 1**__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Training and testing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### For our project the data to create the machine learning model was obtained from one of the main platforms for Data Scientists (Kaggle). It consists in a Data set of 422419 cells with classifed news along with other data which is not relevant for our purpose (the publisher, the URL, the host name...).\n",
    "### The news are classified in the following categories:\n",
    "#### b : business\n",
    "#### t : science and technology \n",
    "#### e : entertainment \n",
    "#### m : health\n",
    "\n",
    "https://www.kaggle.com/uciml/news-aggregator-dataset/version/1#uci-news-aggregator.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important! To be able to run the code in your computer, the data set file should be downloaded and place it in a folder called data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data set which should be downloded from github\n",
    "path_file= './data/dataset_news.csv'\n",
    "file=pd.read_csv(path_file, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Checking the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.1 Missing values\n",
    "There are 2 missing values in the Publisher field. However, nothing will be done with the missing data as this column is not relevant for the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of missing values in each column.\n",
    "table=pd.DataFrame(file.isna().sum()).rename(columns = {0: 'total'})\n",
    "print(table)\n",
    "file[file.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Shape of the data set\n",
    "file.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.2 Bar graph which shows the number of news of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another column is created in order to translate the meaning of 'b', 't', 'e' and 'h'\n",
    "category2=[]\n",
    "for row in file['CATEGORY']:\n",
    "    if row=='b':\n",
    "        category2.append('business')\n",
    "    elif row=='t':\n",
    "        category2.append('science and technology')\n",
    "    elif row== 'e':\n",
    "        category2.append('entertainment')\n",
    "    else:\n",
    "        category2.append('health')\n",
    "file['CATEGORY2'] = category2\n",
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph which shows the number of news from each class. The entertainment class is the most frequent category in \n",
    "#the data set. Therefore, the model will be more specialized in entertainment news.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_group=file.groupby(by='CATEGORY2').count()\n",
    "file_group['ID'].plot(kind='bar', grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.3 Analyzing the publishers of the news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This shows the media companies which most contribute with news to the data set. These are the top 20.\n",
    "file_group=file.groupby(by='PUBLISHER').count()\n",
    "file_group['ID'].sort_values(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.4 Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function is created to normalize the text, removing punctuation symbols and double spaces. This cleaned data is \n",
    "#saved in a new column ('TEXT').\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove punctuation that is not word-internal (e.g., hyphens, apostrophes)\n",
    "    text = re.sub('\\s\\W',' ',text)\n",
    "    text = re.sub('\\W\\s',' ',text)\n",
    "    \n",
    "    # make sure we didn't introduce any double spaces\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "file['TEXT'] = [normalize_text(text) for text in file['TITLE']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The text is vectorized. For more information related to the text \n",
    "#vectorization see:http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(file['TEXT'])\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(file['CATEGORY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Training and analysis of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1 Stratified K-Fold Cross Validation\n",
    "In some cases, there may be a large imbalance in the response variables. For example, in dataset concerning price of houses, there might be large number of houses having high price. Or in case of classification, there might be several times more negative samples than positive samples. For such problems, a slight variation in the K Fold cross validation technique is made, such that each fold contains approximately the same percentage of samples of each target class as the complete set, or in case of prediction problems, the mean response value is approximately equal in all the folds. This variation is also known as Stratified K Fold.\n",
    "\n",
    "### For more information see:\n",
    "https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f\n",
    "\n",
    "https://towardsdatascience.com/balancing-bias-and-variance-to-control-errors-in-machine-learning-16ced95724db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the data is splitted using the Strtified K fold.\n",
    "kf = StratifiedKFold(n_splits=10,shuffle=True,random_state=42)\n",
    "i=1\n",
    "score_mean=[]\n",
    "\n",
    "for train_index,test_index in kf.split(X,y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    #Naive Bayes model\n",
    "    nb = MultinomialNB()\n",
    "    classifier_model = nb.fit(X_train, y_train)\n",
    "    score = nb.score(X_test, y_test)\n",
    "    score_mean.append(score)\n",
    "    print(score)\n",
    "    i+=1\n",
    "print('The mean of the whole predictions is:',sum(score_mean)/len(score_mean))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2 Confussion matrix representation\n",
    "In the below code, a multiclass confusion matrix is plotted.\n",
    "Each row of the matrix represents the results of prediction for the corresponding class at that row, while each column represents the actual class. The diagonal cells show the number of correct classifications by the trained classifier, while the off diagonal cells represent the misclassified predictions.\n",
    "As it is showed in the below plot, the trained classifies correctly 10462 business news, 14670 entertainment news, 4176 health news and 9828 science and technology news. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of the confussion matrix\n",
    "cm = confusion_matrix(y_test, nb.predict(X_test))\n",
    "cm = np.round(cm,0)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the confussion matrix created in the previous code\n",
    "plt.matshow(cm)\n",
    "\n",
    "list_news= ['business','entertainment','health','science and technology']\n",
    "\n",
    "plt.title('Confusion matrix', x=-0.3, y=1.7, fontsize='25')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label', fontsize='15')\n",
    "plt.xlabel('Predicted label', fontsize='15')\n",
    "\n",
    "\n",
    "tick_marks = np.arange(len(list_news))\n",
    "plt.xticks(tick_marks, list_news, rotation=90)\n",
    "plt.yticks(tick_marks, list_news)\n",
    "\n",
    "#The array of the confussion matrix (cm) is represented on the graph plotted previously.\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm)):\n",
    "        plt.text(j,i, str(cm[i][j]), horizontalalignment='center', color='white' if i==j else 'black')\n",
    "        \n",
    "plt.set_cmap('Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the recall and the precission to the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall, precision and accuracy\n",
    "\n",
    "An additional column for the precision and another row for the recall were added to the matrix.\n",
    "The results of the recall and the precision are showed below:<br>\n",
    "- The recall for the business news is 91% and the precission 90%<br>\n",
    "- The recall for the entertainment news is 96% and the precission 96%<br>\n",
    "- The recall for the health news is 92% and the precission 92%<br>\n",
    "- The recall for the science and technology news is 90% and the precission 91%<br>\n",
    "\n",
    "Moreover, the accurcy, which was also calculated in previous cells is 93%.\n",
    "\n",
    "It can be considered that the trained model is having a good performance as the different analyzed metrics showed good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A column for the recall and for the precision is created.\n",
    "#The precision column is created\n",
    "cm_column_precision=[]\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm)):\n",
    "        if i==j:\n",
    "            cm_column_precision.append([cm[i,j]/sum(cm[:,i])])\n",
    "cm_column_precision=np.asarray(cm_column_precision).reshape(1,len(cm))\n",
    "cm_column_precision=np.round(cm_column_precision,3)\n",
    "\n",
    "#The recall column is created\n",
    "cm_row_recall=[]\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm)):\n",
    "        if i==j:\n",
    "            cm_row_recall.append([cm[i,j]/sum(cm[i])])\n",
    "cm_row_recall=np.asarray(cm_row_recall).reshape(len(cm),1)\n",
    "\n",
    "#the 'cm' and the 'cm_column_precision' is joined together and this new array is called cm_join_1.\n",
    "cm_join_1= np.append(cm, cm_column_precision).reshape(len(cm)+1,len(cm))\n",
    "#The acurracy is added to the cm_row_recall. The accurcy is calculated dividing the true positives/all the predictions.\n",
    "#This new array is called cm_rows_with_accuracy\n",
    "\n",
    "true_positives=[]\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm)):\n",
    "        if i==j:\n",
    "            true_positives.append(cm[i,j])\n",
    "true_positives=sum(true_positives)\n",
    "\n",
    "predictions=[]\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm)):\n",
    "        predictions.append(cm[i,j])\n",
    "predictions=sum(predictions)\n",
    "predictions\n",
    "\n",
    "cm_rows_with_accuracy= np.append(cm_row_recall,true_positives/predictions)\n",
    "#The array is rounded to 3 decimales\n",
    "cm_rows_with_accuracy=np.round(cm_rows_with_accuracy,3)\n",
    "\n",
    "#The 'cm_join_1' is joined with the 'cm_rows_with_accuracy' and the new array is called 'cm_join'\n",
    "join=[]\n",
    "for i in range(len(cm_join_1)):\n",
    "    join.append(np.append(cm_join_1[i], cm_rows_with_accuracy[i]))\n",
    "cm_join=np.asarray(join)\n",
    "cm_join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 'cm_join' data is represented in a plot.\n",
    "plt.matshow(cm_join)\n",
    "\n",
    "list_news_rows= ['business','entertainment','health','science and technology','Recall (%)']\n",
    "list_news_columns= ['business','entertainment','health','science and technology', 'Precision (%)']\n",
    "\n",
    "plt.title('Confusion matrix multiclass', x=-0.3, y=1.7, fontsize='25')\n",
    "plt.ylabel('True label', fontsize='15')\n",
    "plt.xlabel('Predicted label', fontsize='15')\n",
    "\n",
    "tick_marks = np.arange(len(cm_join))\n",
    "plt.xticks(tick_marks, list_news_columns, rotation=90)\n",
    "plt.yticks(tick_marks, list_news_rows)\n",
    "\n",
    "#The data is plotted in the confussion matrix\n",
    "for i in range(len(cm_join)):\n",
    "    for j in range(len(cm_join)):\n",
    "        if i==j and j<len(cm_join)-1:\n",
    "            plt.text(j,i, str(cm_join[i][j]), horizontalalignment='center', fontstyle='oblique', color='white')\n",
    "        elif i==j==len(cm_join)-1:\n",
    "            plt.text(j,i, str(cm_join[i][j]), backgroundcolor='yellow', horizontalalignment='center', fontstyle='oblique', color='black')\n",
    "        else:\n",
    "            plt.text(j,i, str(cm_join[i][j]), horizontalalignment='center', fontstyle='oblique', color='black')\n",
    "        \n",
    "        \n",
    "plt.set_cmap('Blues')\n",
    "#plt.figure(figsize = (100,50))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3 ROC curve for a multiclass classificator\n",
    "\n",
    "The ROC curve shows how the true positive rate (recall) vs False positive rate changes as the threshold is varied.\n",
    "\n",
    "The model’s ROC curve can be quantified by calculating the total Area Under the Curve (AUC), a metric which falls between 0 and 1 with a higher number indicating better classification performance. In the below graph, the entertainment class is the curve with the greater area, meaning the model performance for the entertaiment class is better at achieving a blend of precision and recall. A random classifier (dashed line) achieves an AUC of 0.5.\n",
    "\n",
    "Note that the results obtained in the AUC analysis are pretty good as all values are close to 1. Therefore, the blend between recall and precision is much than satisfactory for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A ROC curve multiclasss\n",
    "# calculate roc curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute ROC curve and ROC area for each class. \n",
    "#To perform the ROC curve, the False positive rate and the True positve rate have to be calculated for each class.\n",
    "classes= ['business','entertainment','health','science and technology']\n",
    "#The y_predict_probabilities are classified with the model created (classifier_model) and the 'X_test' values.\n",
    "y_predict_probabilities = classifier_model.predict_proba(X_test)\n",
    "#False positive rate\n",
    "fpr = dict()\n",
    "#True positive rate\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range (len(classes)):\n",
    "    #As the y_test has just an array, the get_dummies is used to have an array with 4 lists.\n",
    "    #The y_predict_probabilities has already 4 lists in the array. Therefore, no operation is needed.\n",
    "    #The 4 values come from the number of classes and it is used to calculate 4 different values of tpr.\n",
    "    #one for each class.\n",
    "    #The fpr is modified by the different threshold values. To calculate this, the y_test and the y_predict_probabilities\n",
    "    #need to have the same dimensions. For this reason, the get_dummies is performed, in order to add 4 dimensions to\n",
    "    #the y_test\n",
    "    fpr[i], tpr[i], _ = roc_curve(np.array(pd.get_dummies(y_test))[:, i], y_predict_probabilities[:,i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    plt.plot(fpr[i],tpr[i], label= '%s class (AUC = %0.5f)' % (classes [i], roc_auc[i]))\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Multiclass', size=20)\n",
    "plt.legend(loc=\"lower right\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For more informtion about the ROC curve and the threshold see these links:\n",
    "ROC curve\n",
    "    http://benalexkeen.com/scoring-classifier-models-using-scikit-learn/\n",
    "        \n",
    "Threshold and ROC curve\n",
    "https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Let's try the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below code is just for myself understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code sort the letters alphabetically. The 'b' is given the 0 position, the 'e' is given the 1 position, the \n",
    "#'h' is given the 2 position and the 'sc' is given the 3 position.\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([\"b\", \"sc\", \"e\", \"h\"])\n",
    "\n",
    "list(le.inverse_transform([2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "#Text to be tested.\n",
    "text_news=\"\"\"Robbie Rotten, has died aged 43 from cancer. Stefansson was best known for his role on the children's show,\n",
    "which was produced from 2004 to 2014. He was initially diagnosed with pancreatic cancer in 2016, but said it had been \n",
    "removed with surgery. He often shared his treatment and progress with fans online on social media - announcing in March \n",
    "the disease had returned and was inoperable. \\\"It's not until they tell you you're going to die soon that you realise \n",
    "how short life is. Time is the most valuable thing in life because it never comes back. And whether you spend it in the \n",
    "arms of a loved one or alone in a prison cell, life is what you make of it. Dream big,\\\" he posted to Twitter. \n",
    "In June his wife Steinunn Olina revealed the father-of-four's cancer was in its final stages.\"\"\"\n",
    "#A codified label is created.\n",
    "le = preprocessing.LabelEncoder()\n",
    "#We fill the previous codified label with the values.\n",
    "le.fit([\"business\", \"science and technology\", \"entertainment\", \"health\"])\n",
    "#The model is texted with the 'text_news' and an array is going to be obtained for each specific case (business,\n",
    "#science and technology, entertinment, health).\n",
    "#We merge the label which was just created (le) and the category of the 'text_news' is obtained.\n",
    "prediction = nb.predict(vectorizer.transform([text_news]))\n",
    "list(le.inverse_transform(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An analysis is performed in order to know the most important words in ech class of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b : business\n",
    "t : science and technology\n",
    "e : entertainment\n",
    "m : health"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important note: We should use the whole data set to see the most relevant words of each class. However, due to processing restrictions, we are going to use 1500 news of each corpus from our data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The news from each class are placed together in order to obtain the different corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The columns for business, health, science/technology and health are selected.\n",
    "#importn\n",
    "b= file.loc[file['CATEGORY2'] == 'business'].head(1500)['TITLE']\n",
    "t= file.loc[file['CATEGORY2'] == 'science and technology'].head(1500)['TITLE']\n",
    "e= file.loc[file['CATEGORY2'] == 'entertainment'].head(1500)['TITLE']\n",
    "m= file.loc[file['CATEGORY2'] == 'health'].head(1500)['TITLE']\n",
    "\n",
    "#All the news which are in each cell of the data set are selected and joined by its category.\n",
    "d_business=''\n",
    "d_sciencetechnology= ''\n",
    "d_entertainment=''\n",
    "d_health=''\n",
    "\n",
    "for text in b:\n",
    "    d_business=d_business + ' ' + text\n",
    "#All the science/technology news are joined together.\n",
    "for text in t:\n",
    "    d_sciencetechnology= d_sciencetechnology + ' ' + text\n",
    "#All the entertainment news are joined together.\n",
    "for text in e:\n",
    "    d_entertainment= d_entertainment + ' ' + text\n",
    "#All the health news are joined together.\n",
    "for text in m:\n",
    "    d_health= d_health + ' ' + text\n",
    "\n",
    "corpus=[d_business,d_sciencetechnology,d_entertainment,d_health]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.1 tf (*term frequency*)\n",
    "\n",
    "[**tf**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Term_frequency_2) es el peso que indica la frecuencia de un término, es decir, el número de veces que una determinada palabra aparece en un documento. \n",
    "\n",
    "La aproximación más sencilla consiste consiste en asignar como peso para el término $t$ en el documento $d$ del corpus $D$ (denotado como $\\mbox{tf}_{t,d}$) el número de ocurrencias de $t$ en $d$. Es recomendable normalizar esta frecuencia, diviendo el número de ocurrencias entre el número total de palabras de un documento, para no penalizar los documentos breves: $\\mathrm{tf}(t,d) = \\frac{\\mathrm{f}(t, d)}{\\max\\{\\mathrm{f}(w, d):w \\in d\\}}$\n",
    "\n",
    "Vamos a calcularlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf={}\n",
    "i=-1\n",
    "for text in corpus:\n",
    "    sentence=text.split()\n",
    "    for word in sentence:\n",
    "        tf[word]=[0]*len(corpus)\n",
    "\n",
    "for text in corpus:\n",
    "    sentence=text.split()\n",
    "    i= i+1\n",
    "    for word in sentence:\n",
    "        tf[word][i]=sentence.count(word)/len(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.4.2 idf (*inverse document frequency*)\n",
    "\n",
    "Trabajar unicamente con las frecuencias de los términos conlleva un problema: todos los términos presentes en la colección se consideran igualmente relevantes a la hora de discriminar la relevancia de los documentos, atendiendo a sus frecuencias. Y resulta que esto no es verdad. \n",
    "\n",
    "Imaginemos un corpus en el que la frecuencia total de dos términos concretos, *este* y *fonema*, es similar en términos absolutos. La distribución de estos términos a lo largo de la coleccion es seguramente muy diferente. El primero aparece con una distribución uniforme a lo largo del corpus, su capacidad discriminativa es baja y debería penalizarse a la hora de asignar relevancia (como el resto de *stopwords*). El segundo, por el contrario, se concentra principalmente en documentos que hablan de fonología, su capacidad discriminativa es alta y debería ser premiado.\n",
    "\n",
    "Existen mecanismos correctores para incorporar estas penalizaciones y premios en nuestros pesos. Los más habituales pasan por recurrir a la frecuencia de documento $\\mbox{df}_t$, definida como el número de documentos de la colección $D$ que contienen el término $t$: $\\mbox{df}_t = {|\\{d \\in D: t \\in d\\}|}$.\n",
    "\n",
    "Más concretamente, se calcula la frecuencia inversa de documento, o [**idf**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency_2) (*inverse document frequency*), definida como: $\\mbox{idf}_t = \\log {|D|\\over \\mbox{df}_t}$, donde $|D|$ indica el número total de documentos de nuestra colección. De este modo, el **idf** de un término específico pero muy discriminativo será alto, mientras que el de un término muy frecuente a lo largo de la coleccion será bajo.\n",
    "\n",
    "## Calculando df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "df={}\n",
    "for word in tf:\n",
    "    df[word]=0\n",
    "    for text in corpus:\n",
    "        if word in text:\n",
    "            df[word]+=1\n",
    "#The words are sorted by the 'df' value and the top 10 words are printed.\n",
    "m=sorted(df.items(),key=operator.itemgetter(1),reverse=True)\n",
    "m[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores de **df** son números enteros: el número de documentos del corpus que contienen cada uno de los términos.\n",
    "\n",
    "## 1.4.3 Calculando idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "idf={}\n",
    "for word in df:\n",
    "    idf[word]= math.log(len(corpus)/df[word])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4.4 tf.idf\n",
    "\n",
    "[**td.idf**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (*term frequency - inverse document frequency*) es una medida numérica que expresa la relevancia de una palabra de un documento con respecto a una colección de documentos. Es uno de los esquemas de pesado más comunes en las tareas relacionadas con la recuperación de información y la minería de texto.\n",
    "\n",
    "El objetivo de esta métrica es representar los documentos de texto como vectores, ignorando el orden concreto de las palabras pero manteniendo la información relativa a las frecuencias de aparición. \n",
    "\n",
    "El valor de tf-idf de una palabra:\n",
    "\n",
    "- es mayor cuanto más frecuente sea esta palabra dentro de un documento concreto, pero;\n",
    "- es mayor cuando menos común sea la palabra en otros documentos de la colección.\n",
    "\n",
    "Estas dos características premian a los términos que son muy frecuentes en determinados documentos concretos pero poco comunes en general: estos términos pueden considerarse buenos descriptores de un conjunto de documentos. Y a la vez, penalizan aquellos términos que aparecen con mucha frecuencia a lo largo de toda la colección, como las *stopwords*.\n",
    "\n",
    "\n",
    "## Calculando **tf.idf**\n",
    "\n",
    "**tf.idf** se calcula como el producto de dos términos: $\\mathrm{tf.idf}(t, d, D) = \\mathrm{tf}(t, d) \\times \\mathrm{idf}(t, D)$\n",
    "\n",
    "- la frecuencia de un término (tf): el número de veces que una determinada palabra aparece en un documento. \n",
    "\n",
    "- la frecuencia inversa de documento (idf): el logaritmo del número total de documentos en el corpus dividido entre el número de documentos en los que el término aparece.\n",
    "\n",
    "Ya hemos calculado previamente esos valores. Bastará con realizar los productos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b : business\n",
    "t : science and technology\n",
    "e : entertainment\n",
    "m : health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The tf.idf is calculated.\n",
    "tfidf = {}\n",
    "\n",
    "for word in tf:\n",
    "    tfidf[word]=[]\n",
    "    for value in tf[word]:\n",
    "        tfidf[word].append(value*idf[word])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first corpus to be sorted are the words from the business\n",
    "#The words from the business corpus are sorted by its value (tfidf)\n",
    "tfidf_business=sorted(tfidf.items(),key= lambda x : x[1][0],reverse=True)[:50]     \n",
    "#The second corpus to be sorted are the words from the science/technology corpus.\n",
    "#The words from the science/technology corpus are sorted by its value (tfidf)\n",
    "tfidf_sciencetechnology=sorted(tfidf.items(),key= lambda x : x[1][1],reverse=True)[:50]     \n",
    "#The third corpus to be sorted are the words from the entertainment corpus\n",
    "#The words from the entertainment corpus are sorted by its value (tfidf)\n",
    "tfidf_entertainment=sorted(tfidf.items(),key= lambda x : x[1][2],reverse=True)[:50]\n",
    "#The fourth corpus to be sorted are the words from the health corpus.\n",
    "#The words from the health corpus are sorted by its value (tfidf)\n",
    "tfidf_health =sorted(tfidf.items(),key= lambda x : x[1][3],reverse=True)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.5 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the data presented in the following tables.\n",
    "\n",
    "The higher of the **tfidf** value in a specific corpus, the more importance of the word in that corpus.<br>\n",
    "Note that a word is relevant in a corpus, when the word is repeated a lot of times in the corpus itself, and it is not contained in other corpus.\n",
    "Therefore, the different tables show the most important words for the business, entertainment, science/technology and\n",
    "health corpus. Bear in mind that these words have a high value just in one of the corpus, and for the rest of them, the value is low or even 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The top 10 most important words from the business corpus are showed in the below table.\n",
    "n=10\n",
    "y_words_b=[tfidf_business[y][0] for y in range(n)]\n",
    "y_business=[tfidf_business[y][1][0] for y in range(n) ]\n",
    "y_business_sciencetechnolgy=[tfidf_business[y][1][1] for y in range(n) ]\n",
    "y_business_entertainment=[tfidf_business[y][1][2] for y in range(n) ]\n",
    "y_business_health=[tfidf_business[y][1][3] for y in range(n) ]\n",
    "\n",
    "print('\\033[1mThe most important words of the business corpus')\n",
    "\n",
    "pd.DataFrame({'0_Words':y_words_b,'1_Business':y_business, '2_science&technology':y_business_sciencetechnolgy,\n",
    "             '3_entertainment':y_business_entertainment, '4_health':y_business_health})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The top 10 most important words from the science&technology corpus are showed in the below table.\n",
    "\n",
    "y_words_sc=[tfidf_sciencetechnology[y][0] for y in range(n)]\n",
    "y_sciencetechnolgy_business=[tfidf_sciencetechnology[y][1][0] for y in range(n) ]\n",
    "y_sciencetechnolgy=[tfidf_sciencetechnology[y][1][1] for y in range(n) ]\n",
    "y_sciencetechnolgy_entertainment=[tfidf_sciencetechnology[y][1][2] for y in range(n) ]\n",
    "y_sciencetechnolgy_health=[tfidf_sciencetechnology[y][1][3] for y in range(n) ]\n",
    "\n",
    "print('\\033[1mThe most important words of the science & technology corpus')\n",
    "\n",
    "pd.DataFrame({'0_Words':y_words_sc,'1_Business':y_sciencetechnolgy_business, '2_science&technology':y_sciencetechnolgy,\n",
    "             '3_entertainment':y_sciencetechnolgy_entertainment, '4_health':y_sciencetechnolgy_health})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The top 10 most important words from the entertainment corpus are showed in the below table.\n",
    "\n",
    "y_words_e=[tfidf_entertainment[y][0] for y in range(n)]\n",
    "y_entertainment_business=[tfidf_entertainment[y][1][0] for y in range(n) ]\n",
    "y_entertainment_sciencetechnolgy=[tfidf_entertainment[y][1][1] for y in range(n) ]\n",
    "y_entertainment=[tfidf_entertainment[y][1][2] for y in range(n) ]\n",
    "y_entertainment_health=[tfidf_entertainment[y][1][3] for y in range(n) ]\n",
    "\n",
    "print('\\033[1mThe most important words of the entertainment corpus')\n",
    "\n",
    "pd.DataFrame({'0_Words':y_words_e,'1_Business':y_entertainment_business, '2_science&technology':y_entertainment_sciencetechnolgy,\n",
    "             '3_entertainment':y_entertainment, '4_health':y_entertainment_health})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The top 10 most important words from the health corpus are showed in the below table.\n",
    "\n",
    "\n",
    "y_words_h=[tfidf_health[y][0] for y in range(n)]\n",
    "y_health_business=[tfidf_health[y][1][0] for y in range(n) ]\n",
    "y_health_sciencetechnolgy=[tfidf_health[y][1][1] for y in range(n) ]\n",
    "y_health_entertainment=[tfidf_health[y][1][2] for y in range(n) ]\n",
    "y_health=[tfidf_health[y][1][3] for y in range(n) ]\n",
    "\n",
    "print('\\033[1mThe most important words of the health corpus')\n",
    "\n",
    "pd.DataFrame({'0_Words':y_words_h,'1_Business':y_health_business, '2_science&technology':y_health_sciencetechnolgy,\n",
    "             '3_entertainment':y_health_entertainment, '4_health':y_health})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
